---
title: "Group Project"
author: "Wang Pei"
date: "10/21/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r loading library}
library(Hmisc)
library(corrplot)
library(ggplot2)
library(Rfast)

# feature selection
library(leaps)

# SVM
library(e1071)

# CART tree
library(partykit) 
library(rpart) 
library(rpart.plot)

# C50 tree
library(C50)

# Bagging
library(caret) 
library(ipred) 

# Random forest
library(randomForest)

# Neural Networks
library(nnet)
library(NeuralNetTools)

library(naivebayes)

library(InformationValue)
```

```{r reading data, echo=TRUE}
card = read.csv("card.csv", header = TRUE, skip = 1)

# Rename pay_0 to pay_1, "default.payment.next.month" as 'target'
names(card)[names(card) == "PAY_0"] <- "PAY_1"
names(card)[names(card) == "default.payment.next.month"] <- "TARGET"

# Configure data types
# Categorical
card$SEX = as.factor(card$SEX)
card$MARRIAGE = as.factor(card$MARRIAGE)
card$TARGET = as.factor(card$TARGET)
# ordinal
card$EDUCATION = as.factor(card$EDUCATION)
```

```{r data information}
describe(card[,c('LIMIT_BAL')]) # Broad range
hist(card$LIMIT_BAL,col="lightblue", main = "Histogram of limit of balance", xlab="limit of balance")
barplot(table(card$SEX,card$LIMIT_BAL))
```

```{r Data exploration}
# Categorical data
describe(card[,c('SEX', 'EDUCATION', 'MARRIAGE')]) 
barplot(table(card$SEX),col= c("lightblue", "red"))
barplot(table(card$TARGET, card$SEX), col=c("lightblue", "red"),beside = TRUE)
barplot(table(card$TARGET,card$EDUCATION),col=rainbow(2),legend("topright",fill=c("lightblue", "red"),legend=c("0","1")))

barplot(table(card$EDUCATION),col=rainbow(3))
barplot(table(card$TARGET, card$EDUCATION), col=c("lightblue", "red"),beside = TRUE)
barplot(table(card$TARGET,card$EDUCATION),col=c("lightblue", "red"),legend("topright",fill=c("lightblue", "red"),legend=c("0","1")))

barplot(table(card$MARRIAGE),,col=rainbow(3))
barplot(table(card$TARGET, card$MARRIAGE), col=c("lightblue", "red"),beside = TRUE)
barplot(table(card$TARGET,card$MARRIAGE),col=c("lightblue", "red"),legend("topright",fill=c("lightblue", "red"),legend=c("0","1")))
# SEX looks fine
# Education: 0 undocumented. 5,6 others
# Marriage: 0 undocumented. 3 others
```

```{r data}
describe(card$AGE)
hist(card$AGE, main="Histogram of AGE", xlab="AGE", col = "lightblue")
```

```{r data}
describe(card[,c('PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6')]) 
hist(card[,c('PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6')], col="lightblue")\

# -2 undo
# -2, -1, 0 should be combined as 0
```


```{r data}
describe(card[,c('BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6')]) 
# Negative values can be interpreted as credit? Has to be investigated
hist(card[,c('BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6')])
```

```{r data}
describe(card[,c('PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6')]) 

hist(card[,c('PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6')])
hist(log(card[,c('PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6')]))
```

```{r data}
describe(card$TARGET) 

barplot(table(card$TARGET),col=c("lightblue", "red"))

```

```{r Data cleaning}
# Education: combine 0, 5, 6 -> 5
# Marriage: 0 -> 3
# PAY_i: -2, -1, 0 -> 0
# Check large pay_amt for outliers
card$EDUCATION[card$EDUCATION==0 | card$EDUCATION==6] = 5
card$MARRIAGE[card$MARRIAGE==0] = 3
card$EDUCATION = factor(card$EDUCATION)
card$MARRIAGE = factor(card$MARRIAGE)
card$PAY_1[card$PAY_1==-2 | card$PAY_1==-1] = 0
card$PAY_2[card$PAY_2==-2 | card$PAY_2==-1] = 0
card$PAY_3[card$PAY_3==-2 | card$PAY_3==-1] = 0
card$PAY_4[card$PAY_4==-2 | card$PAY_4==-1] = 0
card$PAY_5[card$PAY_5==-2 | card$PAY_5==-1] = 0
card$PAY_6[card$PAY_6==-2 | card$PAY_6==-1] = 0


describe(card[,c("EDUCATION","MARRIAGE")])
barplot(table(card$EDUCATION), col=rainbow(5))
barplot(table(card$MARRIAGE),col=rainbow(3))
describe(card[,c('PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6')])
hist(card[,c('PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6')])
```

```{r feature engineering, echo=TRUE}

res <- cor(as.matrix(card[,-c(1,3,4,5,25)]))
# Use rcorr from Hmisc
# res <- rcorr(as.matrix(train.data[,1:7]))
corrplot(res, method="circle")
corrplot(res, method="number")
# Bill_AMT 1 - 6 high correlation

card2 = card
card2$PAY_MAX = apply(card[,c("PAY_1","PAY_2","PAY_3","PAY_4","PAY_5","PAY_6")],1,max) 
card2$BILL_AMT_AVG = rowMeans(card[,13:18])
card2$PAY_AMT_AVG = rowMeans(card[,19:24])
card2[,13:24] = NULL
res = cor(as.matrix(card2[,-c(1,3,4,5,13)]))
corrplot(res, method="circle")

hist(card2$PAY_AMT_AVG)
card2$PAY_AMT_AVG_LOG = log(card2$PAY_AMT_AVG + 1 - min(card2$PAY_AMT_AVG))
hist(card2$PAY_AMT_AVG_LOG)

hist(card2$BILL_AMT_AVG)
card2$BILL_AMT_AVG_LOG = log(card2$BILL_AMT_AVG + 1 - min(card2$BILL_AMT_AVG))
hist(card2$BILL_AMT_AVG_LOG)

```

```{r data split}
set.seed(666)

index = 1:nrow(card)
n = nrow(card)
testIndex = sample(index, trunc(n)/3)

testSet = card2[testIndex,]
trainSet = card2[-testIndex,]

train.target = trainSet$TARGET
test.target = testSet$TARGET

```



```{r feature selection}
set.seed(666)
backward = regsubsets(train.target ~ .,trainSet[,-c(1,3,4,5,6,7)],method = "backward")
summary(backward)
plot(backward,scale="r2")

forward = regsubsets(train.target ~ .,trainSet[,-c(1,3,4,5,6,7)],method = "forward")
summary(forward)
plot(forward,scale="r2")


# backward: limit_bal, Sex, education, pay1, pay4, pay6,bill_Amt3,pay_amt4
# forward: limit_bal, marriage, pay 1,4,6, bill_amt3,pay_amt4

# card2 = card
# card2$PAY_2 = NULL
# card2$PAY_3 = NULL
# card2$PAY_5 = NULL
# card2$BILL_AMT1 = NULL
# card2$BILL_AMT2 = NULL
# card2$BILL_AMT4 = NULL
# card2$BILL_AMT5 = NULL
# card2$BILL_AMT6 = NULL
# card2$PAY_AMT1 = NULL
# card2$PAY_AMT2 = NULL
# card2$PAY_AMT3 = NULL
# card2$PAY_AMT5 = NULL
# card2$PAY_AMT6 = NULL
# 
# set.seed(666)
# 
# index = 1:nrow(card2)
# n = nrow(card2)
# testIndex = sample(index, trunc(n)/3)
# 
# testSet = card2[testIndex,]
# trainSet = card2[-testIndex, ]
# 
# train.target = trainSet$TARGET
# test.target = testSet$TARGET
# 
# for(i in 2:11) {
#   if(is.numeric(trainSet[,i])) {
#     trainSet[,i] = (trainSet[,i] - mean(trainSet[,i])) / sd(trainSet[,i])
#     testSet[,i] = (testSet[,i] - mean(testSet[,i])) / sd(testSet[,i])
#   }
# }

# backward: limit_bal, education, pay1, pay4, pay6,bill_Amt3,pay_amt4
# forward: limit_bal, marriage, pay 1,4,6, bill_amt3,pay_amt4
```
```{r Select features}
chistat = matrix(0,8,2)

names = names(card2)
var = rep("", 23)
col = ncol(card) - 2
class = card$TARGET
for(i in 1:10) {
  if(is.factor(card[,i])) {
    x = as.factor(card[,i])
    tbl = table(x, class)
    cat("\n Attribute =", i, names[i], "\n")
    print(tbl)
    chi2res = chisq.test(tbl)
    print(chi2res)
    chistat[i,1] = chi2res$statistic
    chistat[i,2] = chi2res$p.value
  }
}
chistat


```

```{r Logistic regression}

train_total = 0
test_total = 0

glm = glm(train.target ~ ., data = trainSet[,-c(1,6,7,9,10)],family = "binomial")
summary(glm)

train.glm = predict(glm, newdata = trainSet[,-c(1,6,7,9,10)])
plotROC(train.target, train.glm)
  
test.glm = predict(glm, newdata = testSet[,-c(1,6,7,9,10)])
plotROC(test.target, test.glm)

for (s in 600:700) {
  set.seed(s)
  
  index = 1:nrow(card)
  n = nrow(card)
  testIndex = sample(index, trunc(n)/3)
  
  testSet = card2[testIndex,]
  trainSet = card2[-testIndex,]
  
  train.target = trainSet$TARGET
  test.target = testSet$TARGET
  
  glm = glm(train.target ~ ., data = trainSet[,-c(1,7,9,10)],family = "binomial")
  summary(glm)
  
  # Significant features: limit_BAL, sex, education, marriage, pay_1,3,6, bill_amt1,2, pay_amt1,2,6
  
  train.glm = predict(glm, newdata = trainSet[,-c(1,7,9,10)]) # using log
  # plotROC(train.target, train.glm)
  train.glm.bin = ifelse(predict(glm, newdata = trainSet[,-c(1,7,9,10)],type="response")>0.5,1,0)
  train_total = train_total + mean(train.glm.bin == train.target)
  
  test.glm = predict(glm, newdata = testSet[,-c(1,7,9,10)])
  # plotROC(test.target, test.glm)
  test.glm.bin = ifelse(predict(glm, newdata = testSet[,-c(1,7,9,10)],type="response")>0.5,1,0)
  test_total = test_total + mean(test.glm.bin == test.target)
  # 0.7885
  
}

train_total/100 # 0.7980
test_total/100 # 0.7975

```

```{r SVM}
library(e1071)
# set.seed(666)
# svm.linear <- svm(train.target ~ ., data=trainSet[,2:11], type="C-classification", kernel="linear")
# 
# train.res <- predict(svm.linear,data = trainSet[,2:11])
# test.res <- predict(svm.linear, newdata = testSet[,2:11] )
# mean(train.res == train.target) # 0.8229
# mean(test.res == test.target) # 0.8162
# 
# set.seed(666)
# svm.linear <- svm(train.target ~ ., data=trainSet[,2:11], type="C-classification", kernel="polynomial")
# 
# train.res <- predict(svm.linear,data = trainSet[,2:11])
# test.res <- predict(svm.linear, newdata = testSet[,2:11] )
# mean(train.res == train.target) # 0.8211
# mean(test.res == test.target) # 0.8144
# 
# set.seed(666)
# svm.linear <- svm(train.target ~ ., data=trainSet[,2:11], type="C-classification", kernel="sigmoid")
# 
# train.res <- predict(svm.linear,data = trainSet[,2:11])
# test.res <- predict(svm.linear, newdata = testSet[,2:11] )
# mean(train.res == train.target) # 0.7426
# mean(test.res == test.target) # 0.7431

train_total = 0
test_total = 0
  
for(s in 600:700) {
  set.seed(s)
  
  index = 1:nrow(card4)
  n = nrow(card4)
  testIndex = sample(index, trunc(n)/3)
  
  testSet = card4[testIndex,]
  trainSet = card4[-testIndex,]
  
  train.target = trainSet$TARGET
  test.target = testSet$TARGET
  
  svm.radial <- svm(train.target ~ ., data=trainSet[,-c(1,13,14,15,16)], type="C-classification", kernel="radial")
  
  train.res <- predict(svm.radial,data = trainSet[,-c(1,13,14,15,16)])
  test.res <- predict(svm.radial, newdata = testSet[,-c(1,13,14,15,16)] )
  train_total = train_total + mean(train.res == train.target) # 0.824
  test_total = test_total + mean(test.res == test.target) # 0.818
}

train_total/100
test_total/100
```

```{r CART Tree}

# Recursive partitioning method
# Uses Gini index to calculate impurity index
# Try two different values of the complexity
# control parameter (cp)
cart.tree1 = rpart(train.target ~ ., data = trainSet[,2:11], model=TRUE,method='class',control=rpart.control(cp=0.0001))
train.cart1 = predict(cart.tree1, newdata=trainSet[,2:11],type="class")
table(train.cart1,train.target)
rpart.plot(cart.tree1)
test.cart1 = predict(cart.tree1, newdata=testSet[,2:11],type="class")
mean(train.cart1 == train.target) # 0.8522
mean(test.cart1 == test.target) # 0.7904

cart.tree2 = rpart(train.target ~ ., trainSet[,2:11], model=TRUE,method='class',control=rpart.control(cp=0.01))
train.cart2 = predict(cart.tree2, newdata=trainSet[,2:11],type="class")
test.cart2 = predict(cart.tree2, newdata=testSet[,2:11],type="class")
table(train.cart2,train.target)
rpart.plot(cart.tree2)

plot(cart.tree2)
text(cart.tree2,cex=1.2,use.n=TRUE,xpd=TRUE)

mean(train.cart2 == train.target) # 0.8207
mean(test.cart2 == test.target) # 0.8174
```
```{r CHAID}
library(CHAID)

# MUST have factor variables
vote = lapply(vote,factor)
chaid.tree = chaid(train.target ~ ., data = trainSet[,2:11])
chaid.fit = predict(chaid.tree, newdata=trainSet[,2:11], type="response")
table(chaid.fit, class)
plot(chaid.tree)

```

```{r C50 Tree}
library(C50)
vote = lapply(vote,factor)
C50tree = C5.0(class ~ ., data=vote[vars])
summary(C50tree)
plot(C50tree, type='s')
```

```{r Bagging}
set.seed(666)
bagging = bagging(train.target ~ ., nbagg=25,data=trainSet[,2:11])

train.bagging = predict(bagging, newdata=trainSet[,2:11]) 
mean(train.bagging == train.target) # 0.9928

test.bagging = predict(bagging, newdata=testSet[,2:11]) 
mean(test.bagging == test.target) # 0.7999 SUPER over-fit

varImp(bagging)
```

```{r Random forest}
set.seed(666)
rf = randomForest(train.target ~ .,  data = trainSet[,2:11])
print(rf) # R^2 = 85.95%
plot(rf)

train.rf = predict(rf, newdata = trainSet[,2:11])
test.rf = predict(rf, newdata = testSet[,2:11])

mean(train.rf == train.target) # 0.9319
mean(test.rf == test.target) # 0.8164

```

```{r gbm}
library(gbm)
set.seed(666)
gbm = gbm(train.target ~ .,  data = trainSet[,2:11],distribution = "bernoulli", n.trees = 100, shrinkage = 0.07)

train.gbm = predict(gbm, data = trainSet[,2:11],type = "response")
test.gbm = predict(gbm, newdata = testSet[,2:11], type="response")

mean(train.gbm == train.target) 
mean(test.gbm == test.target)

```

```{r Naive bayes}
bayes = naive_bayes(train.target ~ ., trainSet[,2:11])
train.bayes = predict(bayes, newdata = trainSet[,2:11])
test.bayes = predict(bayes, newdata = testSet[,2:11])

mean(train.bayes == train.target) # 0.7911
mean(test.bayes == test.target) # 787
```

```{r General Linear Regression}
set.seed(666)
glm = glm(train.target ~ ., data=trainSet[,2:11],family = "binomial")
train.glm = predict(glm, newdata=trainSet[,2:11],type="response")
test.glm = predict(glm, newdata=testSet[,2:11],type="response")
train.glm.bin = ifelse(train.glm > 0.5, 1, 0)
test.glm.bin = ifelse(test.glm > 0.5, 1, 0)

mean(train.glm.bin == train.target) # 0.82095
mean(test.glm.bin == test.target) # 0.8141
```

```{r NNet}
ntrain = nrow(trainSet)
# 
library(data.table)
library(mltools)
trainSetOHE <- one_hot(as.data.table(trainSet))
TARGET_train = trainSetOHE$TARGET_1
testSetOHE <- one_hot(as.data.table(testSet))
TARGET_test = testSetOHE$TARGET_1

rmse <- NULL
testacc <- NULL
iteration <- NULL
for (sz in seq(20,30, by = 2)) {
  set.seed(666)
  nn <- nnet(TARGET_train ~ .,data=trainSetOHE[,2:21],size=sz,entropy=TRUE,decay=5e-4,maxit=350) 
  SSresiduals <- apply(nn$residuals,1:2,function(x) x*x)
  rmse0 <- sqrt(sum(SSresiduals)/(2*ntrain-1))
  rmse <- append(rmse,rmse0)
  test.pred <- predict(nn,newdata=testSetOHE[,2:21],type=c("class"))
  testacc <- append(testacc,mean(TARGET_test == test.pred))
  iteration <- append(iteration,Niters)
}

```

```{r plot nnet}
obs <- data.frame(cbind(iteration,rmse,testacc))
p <- ggplot(obs, aes(x = iteration))
p <- p + geom_point(aes(y=rmse)) + geom_line(aes(y = rmse , color = "Train data RMSE")) 
p <- p + geom_point(aes(y=testacc)) + geom_line(aes(y = testacc, color = "Test data ACC")) 
p <- p + scale_y_continuous(sec.axis = sec_axis(~. *100, name = "Test data ACC [%]"))
p <- p + scale_color_manual(values = c("blue", "red"))
p <- p + labs(y = "Train data RMSE",
x = "Iterations",
color = "Parameter")
p <- p + theme(legend.position = c(0.8, 0.5))
p


rmse
testacc
min(rmse)
max(testacc)

```